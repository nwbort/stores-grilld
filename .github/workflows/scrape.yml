# FILE: .github/workflows/scrape.yml
name: Scrape

on:
  push:
  workflow_dispatch:
  schedule:
  # Daily at 6:23 AM UTC
  - cron: '23 6 * * *'

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    if: ${{ !github.event.repository.is_template }}
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"

    - name: Install dependencies
      run: pip install -r requirements.txt

    - name: Run the scraper
      run: python3 scrape.py
      
    - name: Upload debug artifacts
      if: always() # This will run even if the scraper step fails or succeeds
      uses: actions/upload-artifact@v4
      with:
        name: debug-html-pages
        path: debug_html/
        if-no-files-found: ignore # Don't fail if no debug files were created

    - name: Commit and push if there are changes
      run: |-
        git config user.name "Automated"
        git config user.email "actions@users.noreply.github.com"
        git add -A
        timestamp=$(date -u)
        git commit -m "Scraped data: ${timestamp}" || exit 0
        git pull --rebase
        git push
